# Dynamic Batching for Image Generation

## Overview

The image generation service now supports **dynamic request batching** to dramatically improve throughput when handling concurrent users. This feature groups multiple image generation requests together and processes them in a single GPU inference pass, providing 2-3x throughput improvement.

## The Problem

Without batching, each image generation request processes sequentially:
- **Single Request:** ~15 seconds per image
- **50 Concurrent Users:** 50 Ã— 15s = 750 seconds (12.5 minutes for last user) âŒ

This creates unacceptable wait times as more users join the game.

## The Solution: Intelligent Batching

The batching system intelligently groups requests:
- Collects requests for a configurable time window (default: 5 seconds)
- Processes up to 8 images in a single GPU batch
- **Batch of 8:** ~40 seconds total (5 images/second throughput) âœ…
- **50 Users with Batching:** ~4 minutes for last user (3x improvement!)

### Key Features

1. **Dynamic Batch Formation**
   - Automatically groups concurrent requests
   - Configurable batch size (1-16 depending on GPU memory)
   - Configurable wait time window

2. **Adaptive Strategy**
   - Reduces wait time when queue is deep
   - Balances latency vs throughput automatically
   - No manual tuning required

3. **Comprehensive Monitoring**
   - Real-time queue depth tracking
   - Batch size and wait time metrics
   - Throughput and performance statistics

## Configuration

### Environment Variables

All batching configuration is controlled via environment variables:

```bash
# Enable batching (default: true)
ENABLE_BATCHING=true

# Max requests per batch
# AUTO-DETECTED based on available GPU VRAM!
# The service automatically calculates optimal batch size:
#   - RTX 3060 (12GB): Auto-detects 2
#   - RTX 3090 (24GB): Auto-detects 4-5
#   - RTX 4090 (24GB): Auto-detects 8
#   - L40S (48GB): Auto-detects 12-13
#   - A100 (80GB): Auto-detects 20+
#   - H100 (80GB): Auto-detects 20+
#
# Manual override (optional):
# MAX_BATCH_SIZE=8

# Batch wait time in seconds (default: 5.0)
# Lower = faster single requests
# Higher = better batching efficiency
MAX_BATCH_WAIT_TIME=5.0

# Enable adaptive batching (default: true)
# Automatically reduces wait time when queue is deep
ENABLE_ADAPTIVE_BATCHING=true

# Queue threshold for adaptive mode (default: 10)
# When queue > threshold, wait times are reduced
ADAPTIVE_BATCH_THRESHOLD=10
```

### Multi-GPU Configuration

The service **automatically detects and uses all available GPUs** when batching is enabled:

```bash
# Specify which GPUs to use (auto-detects all GPUs if not set)
GPU_IDS=0,1,2,3

# Or use CUDA_VISIBLE_DEVICES (system-level)
CUDA_VISIBLE_DEVICES=0,1,2,3

# Load balancing strategy
# "round-robin" = distribute requests evenly across GPUs
# "queue-depth" = send to GPU with shortest queue
GPU_LOAD_BALANCE_STRATEGY=queue-depth
```

**How it works:**
- Service detects all available GPUs at startup
- Creates one batched backend per GPU
- Distributes incoming requests across GPUs
- Each GPU processes its own batch queue independently
- **Result:** Near-linear scaling with GPU count!

### Quick Start

1. **Enable batching in your environment:**
   ```bash
   export ENABLE_BATCHING=true
   export MAX_BATCH_SIZE=8
   export MAX_BATCH_WAIT_TIME=5.0
   ```

2. **Start the service:**
   ```bash
   uvicorn app.main:app --host 0.0.0.0 --port 8080
   ```

3. **Monitor performance:**
   ```bash
   curl http://localhost:8080/stats
   ```

## Performance Metrics

### Throughput Comparison

| Configuration | Single Request | 50 Concurrent Requests |
|--------------|----------------|------------------------|
| No Batching | ~15s | ~750s (12.5 min) |
| Batch Size 4 | ~15s | ~375s (6.25 min) |
| Batch Size 8 | ~15s | ~250s (4.2 min) |
| Batch Size 8 Ã— 4 GPUs | ~15s | ~62s (1 min) |

### GPU Utilization

Without batching:
- Single inference uses ~30-40% of GPU cores
- Memory bandwidth underutilized
- Wasted compute capacity

With batching:
- Batch inference uses ~70-90% of GPU cores
- Better memory bandwidth utilization
- 2-3x effective throughput increase

## Scaling Strategy

### For 50+ Concurrent Users

To achieve <1 minute response times for 50 users:

**Option 1: Multiple L40S GPUs (with Multi-GPU Batching)**
- **2Ã— L40S:** ~125 seconds for 50 images
- **4Ã— L40S:** ~62 seconds for 50 images
- **8Ã— L40S:** ~31 seconds for 50 images âœ…
- Combined throughput scales linearly with GPU count

**Option 2: H100 GPUs (with Multi-GPU Batching)**
- **2Ã— H100:** ~39 seconds for 50 images
- **4Ã— H100:** ~19 seconds for 50 images âœ…
- **8Ã— H100:** ~10 seconds for 50 images ðŸš€
- ~3x faster per-GPU than L40S

**Option 3: Mixed GPU Setup**
- Use whatever GPUs you have available
- System automatically detects and uses all GPUs
- Linear scaling: 2 GPUs = 2x throughput, 4 GPUs = 4x throughput

### Multi-GPU Scaling Performance

| GPUs | GPU Type | Batch Size Each | Combined Throughput | 50 Images Time |
|------|----------|----------------|---------------------|----------------|
| 1 | L40S | 8 | 0.20 img/sec | 250 sec (4.2 min) |
| 2 | L40S | 8 | 0.40 img/sec | 125 sec (2.1 min) |
| 4 | L40S | 8 | 0.80 img/sec | 62 sec (1.0 min) |
| 8 | L40S | 8 | 1.60 img/sec | 31 sec |
| 2 | H100 | 16 | 1.28 img/sec | 39 sec |
| 4 | H100 | 16 | 2.56 img/sec | 19 sec |
| 8 | H100 | 16 | 5.12 img/sec | 10 sec âœ… |

### Monitoring and Tuning

#### Check Statistics

**Single GPU Response:**
```bash
curl http://localhost:8080/stats | jq
```

```json
{
  "total_requests": 150,
  "total_batches": 23,
  "current_queue_depth": 5,
  "max_queue_depth": 47,
  "avg_batch_size": 6.5,
  "avg_wait_time_ms": 2341,
  "avg_generation_time_ms": 38500,
  "config": {
    "max_batch_size": 8,
    "max_wait_time": 5.0,
    "adaptive_batching": true
  }
}
```

**Multi-GPU Response:**
```bash
curl http://localhost:8080/stats | jq
```

```json
{
  "num_gpus": 4,
  "gpu_ids": [0, 1, 2, 3],
  "load_balance_strategy": "queue-depth",
  "total_requests": 600,
  "total_batches": 92,
  "max_queue_depth": 47,
  "avg_batch_size": 6.5,
  "avg_wait_time_ms": 2341,
  "avg_generation_time_ms": 38500,
  "per_gpu_stats": [
    {
      "gpu_id": 0,
      "current_queue_depth": 2,
      "total_requests": 150,
      "total_batches": 23,
      "avg_batch_size": 6.5
    },
    {
      "gpu_id": 1,
      "current_queue_depth": 1,
      "total_requests": 148,
      "total_batches": 23,
      "avg_batch_size": 6.4
    },
    {
      "gpu_id": 2,
      "current_queue_depth": 3,
      "total_requests": 152,
      "total_batches": 23,
      "avg_batch_size": 6.6
    },
    {
      "gpu_id": 3,
      "current_queue_depth": 0,
      "total_requests": 150,
      "total_batches": 23,
      "avg_batch_size": 6.5
    }
  ],
  "config": {
    "max_batch_size": 8,
    "max_wait_time": 5.0,
    "adaptive_batching": true
  }
}
```

#### Key Metrics to Watch

1. **avg_batch_size**
   - Target: Close to max_batch_size
   - Low value (<50% of max): Increase wait time or reduce batch size
   - High value (>90% of max): Good batching efficiency

2. **avg_wait_time_ms**
   - Should be less than max_wait_time
   - If consistently at max: Queue might be too deep, consider scaling

3. **current_queue_depth**
   - Sustained high depth: Add more GPU workers
   - Frequently zero: Consider reducing wait time

4. **max_queue_depth**
   - Peak load indicator
   - Use to plan capacity for worst-case scenarios

## Load Testing

### Simulate Concurrent Users

```bash
# Install dependencies
pip install httpx

# Test with 50 simultaneous requests
python test_batching.py --burst 50

# Test sustained load (10 users Ã— 5 requests each)
python test_batching.py --users 10 --requests-per-user 5
```

### Expected Results

**50 Burst Test (L40S, batch size 8):**
```
Total Duration: 258s
Successful: 50/50
Avg batch size: 7.8
Throughput: 0.19 images/second
```

**Without Batching:**
```
Total Duration: 750s
Throughput: 0.067 images/second
```

## Troubleshooting

### Issue: Low Batch Sizes

**Symptoms:** avg_batch_size < 3, even with many concurrent users

**Solutions:**
- Increase MAX_BATCH_WAIT_TIME (try 7-10 seconds)
- Check that ENABLE_BATCHING=true
- Verify requests arrive within the wait window

### Issue: High Wait Times

**Symptoms:** avg_wait_time_ms consistently > 10 seconds

**Solutions:**
- Reduce MAX_BATCH_WAIT_TIME
- Enable ENABLE_ADAPTIVE_BATCHING=true
- Lower ADAPTIVE_BATCH_THRESHOLD

### Issue: GPU Out of Memory

**Symptoms:** CUDA out of memory errors, service crashes

**Solutions:**
- Reduce MAX_BATCH_SIZE (try 4 or 6)
- Check GPU memory: `nvidia-smi`
- Verify no other processes using GPU

### Issue: Queue Depth Growing

**Symptoms:** current_queue_depth keeps increasing, never decreases

**Solutions:**
- **Immediate:** Restart service to clear queue
- **Short-term:** Reduce batch size to process faster
- **Long-term:** Add more GPU workers

## API Reference

### POST /generate

Generate an image (automatically batched).

**Request:**
```json
{
  "prompt": "A beautiful landscape",
  "game_id": "game-123",
  "round_id": "round-1",
  "player_id": "player-42",
  "width": 1024,
  "height": 1024,
  "num_inference_steps": 25,
  "guidance_scale": 7.5
}
```

**Response:**
```json
{
  "image_id": "uuid",
  "image_url": "http://...",
  "duration_ms": 38420,
  ...
}
```

### GET /stats

Get batch processing statistics.

**Response:**
```json
{
  "total_requests": 150,
  "total_batches": 23,
  "current_queue_depth": 5,
  "avg_batch_size": 6.5,
  "avg_wait_time_ms": 2341,
  "avg_generation_time_ms": 38500,
  "config": {...}
}
```

## Architecture Details

### Request Flow

```
1. Client sends POST /generate
   â†“
2. Request added to pending queue
   â†“
3. Background processor checks queue every 100ms
   â†“
4. Decision: Process now?
   - YES if: queue â‰¥ max_batch_size OR (queue â‰¥ min_batch_size AND age â‰¥ wait_time)
   - NO: Continue waiting
   â†“
5. Extract batch (up to max_batch_size requests)
   â†“
6. Run batch inference on GPU
   - All prompts processed in single forward pass
   - ~2-3x faster than sequential processing
   â†“
7. Distribute results to waiting requests
   â†“
8. Update statistics
   â†“
9. Return to step 3
```

### Adaptive Wait Time Calculation

```python
if queue_depth >= adaptive_threshold:
    ratio = min(1.0, queue_depth / (adaptive_threshold * 2))
    wait_time = max_wait_time - (ratio * (max_wait_time - min_wait_time))
else:
    wait_time = max_wait_time
```

When queue is deep, wait time decreases linearly to process faster.

## Future Enhancements

Potential improvements for future versions:

1. **Multi-GPU Support**
   - Distribute batches across multiple GPUs
   - Automatic GPU selection and load balancing

2. **Priority Queues**
   - VIP users get faster processing
   - Time-sensitive requests jump the queue

3. **Dynamic Batch Sizing**
   - Automatically adjust batch size based on queue depth
   - Smaller batches when queue is short (lower latency)
   - Larger batches when queue is deep (higher throughput)

4. **Predictive Batching**
   - Learn usage patterns
   - Pre-form batches based on expected load

5. **Cross-Node Batching**
   - Batch requests across multiple service instances
   - Centralized queue with distributed workers

## Support

For issues or questions:
- Check logs for error messages
- Review statistics endpoint for performance metrics
- Adjust configuration based on your GPU and load patterns
- Consider scaling horizontally for high-concurrency scenarios
