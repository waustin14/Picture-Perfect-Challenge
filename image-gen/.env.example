# Image Generation Service Configuration

# ============================================================================
# Generation Mode
# ============================================================================
# Options: "local" (uses local Stable Diffusion), "stability" (Stability AI API), "mock" (placeholder images)
GEN_MODE=local

# ============================================================================
# Batching Configuration (Local Backend Only)
# ============================================================================

# Enable/disable request batching for improved throughput
# When enabled, concurrent requests are grouped and processed together on the GPU
# This can provide 2-3x throughput improvement for concurrent users
ENABLE_BATCHING=true

# Maximum number of requests to batch together
# If not set, automatically calculated based on available GPU VRAM:
#   - RTX 3060 (12GB): Auto-detects batch size 2
#   - RTX 4090 (24GB): Auto-detects batch size 8
#   - L40S (48GB): Auto-detects batch size 12-13
#   - H100 (80GB): Auto-detects batch size 20+
# Set manually to override auto-detection
# MAX_BATCH_SIZE=8

# Minimum batch size before processing
# Set to 1 to process single requests immediately when queue is empty
MIN_BATCH_SIZE=1

# Maximum time to wait for batch to form (seconds)
# Requests wait up to this long for more requests to arrive before processing
# Lower = faster response for single requests, higher = better batching efficiency
MAX_BATCH_WAIT_TIME=5.0

# Minimum wait time when queue is deep (seconds)
# Used with adaptive batching to reduce latency under heavy load
MIN_BATCH_WAIT_TIME=0.5

# Enable adaptive batching strategy
# When enabled, wait time decreases as queue depth increases
# This balances latency and throughput under varying load
ENABLE_ADAPTIVE_BATCHING=true

# Queue depth threshold for adaptive batching
# When queue exceeds this size, wait times are reduced
# Recommended: 10-20 for most use cases
ADAPTIVE_BATCH_THRESHOLD=10

# ============================================================================
# Model Configuration (Local Backend)
# ============================================================================

# Hugging Face model ID for Stable Diffusion
MODEL_ID=stabilityai/stable-diffusion-xl-base-1.0

# Device selection: "auto" (auto-detect), "cuda" (force GPU), "cpu" (force CPU)
DEVICE=auto

# ============================================================================
# Multi-GPU Configuration
# ============================================================================

# Specify which GPUs to use (comma-separated list of GPU IDs)
# If not set, will auto-detect all available GPUs
# Example: GPU_IDS=0,1,2 (use GPUs 0, 1, and 2)
GPU_IDS=

# Alternative: Use CUDA_VISIBLE_DEVICES to control GPU visibility
# This is set at the system level before starting the service
# Example: CUDA_VISIBLE_DEVICES=0,2 (only GPUs 0 and 2 are visible)
# CUDA_VISIBLE_DEVICES=

# Force multi-GPU mode even with single GPU (for testing)
FORCE_MULTI_GPU=false

# Load balancing strategy for multi-GPU mode
# Options: "round-robin" (distribute evenly), "queue-depth" (send to least busy GPU)
# Recommended: "queue-depth" for better load balancing under variable load
GPU_LOAD_BALANCE_STRATEGY=round-robin

# ============================================================================
# Stability AI Backend Configuration
# ============================================================================

# API key for Stability AI (required if GEN_MODE=stability)
STABILITY_API_KEY=

# API host (usually not needed to change)
STABILITY_API_HOST=https://api.stability.ai

# Engine ID for generation
STABILITY_ENGINE_ID=stable-diffusion-xl-1024-v1-0

# Allow fallback to mock backend if Stability API fails
ALLOW_STABILITY_FALLBACK=true

# ============================================================================
# Storage Configuration
# ============================================================================

# Local output directory for generated images
OUTPUT_DIR=/data/images

# Public base URL for serving images
PUBLIC_BASE_URL=http://image-gen:8080

# MinIO Configuration
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=game-images
MINIO_PUBLIC_URL=http://localhost:9000

# ============================================================================
# Observability Configuration
# ============================================================================

# OpenTelemetry service name
OTEL_SERVICE_NAME=image-gen

# OTLP exporter endpoint
OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
